{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MagPySV example workflow - European observatories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup python paths and import some modules\n",
    "from IPython.display import Image\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Import all of the MagPySV modules\n",
    "import magpysv.denoise as denoise\n",
    "import magpysv.io as io\n",
    "import magpysv.model_prediction as model_prediction\n",
    "import magpysv.plots as plots\n",
    "import magpysv.tools as tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lib import consume_webservices as cws\n",
    "\n",
    "wdc_app_path = '/Users/gracecox/geomag_wdc_web_app_interface/'\n",
    "sys.path.append(wdc_app_path)\n",
    "cadence = 'hour'\n",
    "\n",
    "start_date = dt.date(1960, 1, 1)\n",
    "end_date = dt.date(2009, 12, 31)\n",
    "service = 'WDC'\n",
    "download_dir = '/Users/gracecox/Desktop/download_test/'\n",
    "configpath = os.path.join(wdc_app_path, 'lib/consume_rest.ini')\n",
    "\n",
    "observatory_list = ['CLF', 'NGK', 'WNG']\n",
    "\n",
    "[\n",
    "cws.fetch_data(\n",
    "        start_date, end_date,\n",
    "        observatory_, cadence,\n",
    "        service, download_dir, configpath\n",
    ")\n",
    "for observatory_ in observatory_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "observatory_list = ['CLF', 'NGK', 'WNG']\n",
    "download_dir = '/Users/gracecox/Desktop/download_test/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract all data from the WDC files, convert into the proper hourly means using the tabular base and save the X, Y and Z components to CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "io.wdc_to_hourly_csv(wdc_path=download_dir, write_path=download_dir + '/hourly/', obs_list=observatory_list,\n",
    "                  print_obs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Path to file containing baseline discontinuity information\n",
    "baseline_data = misc.get_baseline_info(file_path='/Users/gracecox/Desktop/jumps_info/jump_records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loop over all observatories and calculate SV series for each\n",
    "for observatory in observatory_list:\n",
    "    print(observatory)\n",
    "    # Load hourly data\n",
    "    data_file = observatory + '.csv'\n",
    "    hourly_data = io.read_csv_data(\n",
    "        fname=os.path.join(download_dir + 'hourly/', data_file),\n",
    "        data_type='mf')\n",
    "    # Resample to monthly means\n",
    "    resampled_field_data = tools.data_resampling(hourly_data, sampling='MS', average_date=True)\n",
    "    # Correct documented baseline changes\n",
    "    misc.correct_baseline_change(observatory=observatory,\n",
    "                          field_data=resampled_field_data,\n",
    "                          jump_data=jump_data)\n",
    "    # Write out the monthly means for magnetic field\n",
    "    io.write_csv_data(data=resampled_field_data,\n",
    "                            write_path=download_dir + 'monthly_mf/',\n",
    "                            obs_name=observatory)\n",
    "    # Calculate SV from monthly field means\n",
    "    sv_data = tools.calculate_sv(resampled_field_data,\n",
    "                                   mean_spacing=1)\n",
    "    # Write out the SV data\n",
    "    io.write_csv_data(data=sv_data,\n",
    "                               write_path=download_dir + 'monthly_sv/fdmm/',\n",
    "                               obs_name=observatory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Field model predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need some secular variation predictions from a geomagnetic field model. This example uses COV-OBS. The following code obtains the complete list of geomagnetic observatory locations from the WDC website, converts the lat/lon in degrees to colat/lon in radians and altitude from m to km, and then runs the COV-OBS model for each location to produce files containing the model prediction of SV and MF at that location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stations = model_prediction.get_observatory_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_prediction.run_covobs(stations=stations, model_path='/Users/gracecox/Dropbox/cov-obs_x1/',\n",
    "                            output_path='/Users/gracecox/Dropbox/cov-obs_x1/monthly_vals/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate the data for our selected observatories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Start and end dates of the analysis as (year, month, day)\n",
    "start = dt.datetime(1960, 1, 1)\n",
    "end = dt.datetime(2010, 12, 31)\n",
    "\n",
    "obs_data, model_sv_data, model_mf_data = io.combine_csv_data(\n",
    "    start_date=start, end_date=end, obs_list=observatory_list,\n",
    "    data_path=download_dir + 'monthly_sv/fdmm/',\n",
    "    model_path=\"/Users/gracecox/Dropbox/field_models/cov-obs_x1/monthly_vals/\", day_of_month=1)\n",
    "\n",
    "dates = obs_data['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "obs_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SV plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for observatory in observatory_list:\n",
    "    fig = plots.plot_sv(dates=dates, sv=obs_data.filter(regex=observatory),\n",
    "                    model=model_sv_data.filter(regex=observatory),\n",
    "                    fig_size=(6, 18), font_size=10, label_size=16, plot_legend=False,\n",
    "                    obs=observatory, model_name='COV-OBS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Optionally remove spikes in the data before denoising. Large outliers can affect the denoising process so better to remove them beforehand for some series (i.e. at high latitude observatories)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "obs_data.drop(['date'], axis=1, inplace=True)\n",
    "for column in obs_data:    \n",
    "    obs_data[column] = denoise.detect_outliers(dates=dates, signal=obs_data[column], obs_name=column,\n",
    "                                               threshold=4,\n",
    "                                               window_length=96, plot_fig=True, fig_size=(4,4))\n",
    "obs_data.insert(0, 'date', dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# External noise removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the residuals and use the eigenvalues/vectors of the covariance matrix to remove unmodelled external signal (Wardinski & Holme, 2011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "residuals = tools.calculate_residuals(obs_data=obs_data, model_data=model_sv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_sv_data.drop(['date'], axis=1, inplace=True)\n",
    "obs_data.drop(['date'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "denoised, proxy, eigenvals, eigenvecs, projected_residuals, corrected_residuals = denoise.eigenvalue_analysis(\n",
    "    dates=dates, obs_data=obs_data, model_data=model_sv_data, residuals=residuals,\n",
    "    proxy_number=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoised SV plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots showing the original SV data, the denoised data (optionally with a running average) and the field model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for observatory in observatory_list:\n",
    "    xratio, yratio, zratio = plots.plot_sv_comparison(dates=dates, denoised_sv=denoised.filter(regex=observatory),\n",
    "        residuals=residuals.filter(regex=observatory),\n",
    "        corrected_residuals = corrected_residuals.filter(regex=observatory),\n",
    "        noisy_sv=obs_data.filter(regex=observatory), model=model_sv_data.filter(regex=observatory),\n",
    "        model_name='COV-OBS',\n",
    "        fig_size=(10, 7), font_size=10, label_size=14, obs=observatory, plot_rms=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots showing the denoised data (optionally with a running average) and the field model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for observatory in observatory_list:\n",
    "    plots.plot_sv(dates=dates, sv=denoised.filter(regex=observatory), model=model_sv_data.filter(regex=observatory),\n",
    "                    fig_size=(10, 5), font_size=10, label_size=14, plot_legend=False, obs=observatory, model_name='COV-OBS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot proxy signal, eigenvalues and eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the proxy signal used to denoise the data with the Dst index, measures the intensity of the equatorial electrojet (the \"ring current\"). Both signals are reduced to zero-mean and unit variance (z-score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dst_file = '/Users/gracecox/Dropbox/DataDownloads/Dst/dst_fdmm.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plots.plot_index_dft(index_file=dst_file, dates=denoised.date, signal=proxy, fig_size=(10, 8), font_size=10,\n",
    "                       label_size=14, plot_legend=True, index_name='Dst')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the eigenvalues of the covariance matrix of the residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plots.plot_eigenvalues(values=eigenvals, font_size=12, label_size=16, fig_size=(8, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the three eigenvectors corresponding to the three largest eigenvalues. The noisiest direction (used to denoise in this example) is mostly X, with some Z, which is consistent with the ring current for European observatories. The second noisiest direction (also used to denoise in this example) is predominantly Z, with some X, which is again consistent with rind current directions. However, the third noisiest direction is a coherent Y signal across Europe, which does not correspond to a known direction of external signal. We did not remove this direction during denoising as it could be a real internal field variation that is not captured by the field model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plots.plot_eigenvectors(obs_names=observatory_list, eigenvecs=eigenvecs[:,0:3], fig_size=(8, 4),\n",
    "                          font_size=10, label_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove remaining spikes in the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "denoised.drop(['date'], axis=1, inplace=True)\n",
    "for column in denoised:\n",
    "    denoised[column] = denoise.detect_outliers(dates=dates, signal=denoised[column], obs_name=column, threshold=5,\n",
    "                                               window_length=120, plot_fig=False, fig_size=(10, 3), font_size=10, label_size=14)\n",
    "denoised.insert(0, 'date', dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write denoised data to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for observatory in observatory_list:\n",
    "    print(observatory)\n",
    "    sv_data=denoised.filter(regex=observatory)\n",
    "    sv_data.insert(0, 'date', dates)\n",
    "    sv_data.columns = [\"date\", \"dX\", \"dY\", \"dZ\"]\n",
    "    io.write_csv_data(data=sv_data, write_path=download_dir + '/denoised/european/',\n",
    "                               obs_name=observatory, decimal_dates=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Averaging data over Europe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select denoised data for each SV component at all observatories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "obs_X = denoised.filter(regex='dX')\n",
    "model_X = model_sv_data.filter(regex='dX')\n",
    "obs_Y = denoised.filter(regex='dY')\n",
    "model_Y = model_sv_data.filter(regex='dY')\n",
    "obs_Z = denoised.filter(regex='dZ')\n",
    "model_Z = model_sv_data.filter(regex='dZ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average data and model for each component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_X = pd.DataFrame(np.mean(obs_X.values, axis=1))\n",
    "mean_X.columns = ['dX']\n",
    "mean_model_X = np.mean(model_X, axis=1)\n",
    "mean_Y = pd.DataFrame(np.mean(obs_Y.values, axis=1))\n",
    "mean_Y.columns = ['dY']\n",
    "mean_model_Y = np.mean(model_Y, axis=1)\n",
    "mean_Z = pd.DataFrame(np.mean(obs_Z.values, axis=1))\n",
    "mean_Z.columns = ['dZ']\n",
    "mean_model_Z = np.mean(model_Z, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove outliers from averaged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_X = denoise.detect_outliers(dates=dates, signal=mean_X, obs_name='X', threshold=2.5,\n",
    "                                               window_length=72, plot_fig=False, fig_size=(10, 3), font_size=10, label_size=14)\n",
    "mean_Y = denoise.detect_outliers(dates=dates, signal=mean_Y, obs_name='Y', threshold=2.5,\n",
    "                                               window_length=72, plot_fig=False, fig_size=(10, 3), font_size=10, label_size=14)\n",
    "mean_Z = denoise.detect_outliers(dates=dates, signal=mean_Z, obs_name='Z', threshold=2.5,\n",
    "                                               window_length=72, plot_fig=False, fig_size=(10, 3), font_size=10, label_size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at model predictions for all observatories, and the averaged model, to see if the average is representative of the trend at all locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(dates, model_X)\n",
    "plt.plot(dates, mean_model_X, 'k--')\n",
    "legend = model_X.columns.tolist()\n",
    "legend.append('Average')\n",
    "plt.legend(legend, frameon=False)\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(dates, model_Y)\n",
    "plt.plot(dates, mean_model_Y, 'k--')\n",
    "plt.ylabel('SV (nT/yr)',  fontsize=16)\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(dates, model_Z)\n",
    "plt.plot(dates, mean_model_Z, 'k--')\n",
    "plt.xlabel('Year',  fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the averaged data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(dates, mean_X, 'b')\n",
    "plt.plot(dates, np.mean(model_X, axis=1), 'r')\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(dates, mean_Y, 'b')\n",
    "plt.plot(dates, np.mean(model_Y, axis=1), 'r')\n",
    "plt.ylabel('SV (nT/yr)', fontsize=16)\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(dates, mean_Z, 'b', label='Averaged data')\n",
    "plt.plot(dates, np.mean(model_Z, axis=1), 'r', label='Averaged COV-OBS')\n",
    "plt.xlabel('Year',  fontsize=16)\n",
    "plt.legend(loc='best', fontsize=10, frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "#plt.gca().xaxis_date()\n",
    "plt.plot(denoised.date, mean_X, 'b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data selection using the ap index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Select an observatory\n",
    "observatory = 'CLF'\n",
    "data_file = observatory + '.csv'\n",
    "hourly_data = io.read_csv_data(\n",
    "    fname=os.path.join(download_dir + 'hourly/', data_file),\n",
    "    data_type='mf')\n",
    "# Correct documented baseline changes\n",
    "misc.correct_baseline_change(observatory=observatory,\n",
    "                      field_data=hourly_data,\n",
    "                      jump_data=baseline_data)\n",
    "# Apply an Ap criterion to discard noisy data\n",
    "hourly_data_ap = tools.apply_Ap_threshold(obs_data=hourly_data, Ap_file='/Users/gracecox/Dropbox/DataDownloads/Kp_Ap/ap_hourly.csv',\n",
    "                               threshold=7.0)\n",
    "# Resample to monthly means\n",
    "resampled_field_data = tools.data_resampling(hourly_data, sampling='MS', average_date=True)\n",
    "resampled_field_data_ap = tools.data_resampling(hourly_data_ap, sampling='MS', average_date=True)\n",
    "# Calculate SV from monthly field means\n",
    "sv_data = tools.calculate_sv(resampled_field_data,\n",
    "                               mean_spacing=1)\n",
    "sv_data_ap = tools.calculate_sv(resampled_field_data_ap,\n",
    "                               mean_spacing=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hourly_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = hourly_data_ap.date\n",
    "hourly_data_ap.drop(['date'], axis=1, inplace=True)\n",
    "for column in hourly_data_ap:    \n",
    "    hourly_data_ap[column] = denoise.detect_outliers(dates=d, signal=hourly_data_ap[column], obs_name=column,\n",
    "                                               threshold=15,\n",
    "                                               window_length=2400, plot_fig=True, fig_size=(10,3))\n",
    "hourly_data_ap.insert(0, 'date', d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the percentage of data remaining after applying the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hourly_data_ap.X.count()#/hourly_data.X.count() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the hourly magnetic field data before and after appyling the ap threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 9))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(hourly_data.date, hourly_data.X, 'b')\n",
    "plt.plot(hourly_data.date, hourly_data_ap.X, 'r')\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(hourly_data.date, hourly_data.Y, 'b')\n",
    "plt.plot(hourly_data.date, hourly_data_ap.Y, 'r')\n",
    "plt.ylabel('Magnetic Field (nT)', fontsize=16)\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(hourly_data.date, hourly_data.Z, 'b', label='All data')\n",
    "plt.plot(hourly_data.date, hourly_data_ap.Z, 'r', label='ap < 7')\n",
    "plt.xlabel('Year', fontsize=16)\n",
    "plt.legend(frameon=False)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the SV obtained when calculated using all hourly data and hourly the ap threshold applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 12))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(sv_data.date, sv_data.dx, 'b')\n",
    "plt.plot(sv_data_ap.date, sv_data_ap.dx, 'r')\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(sv_data.date, sv_data.dy, 'b')\n",
    "plt.plot(sv_data_ap.date, sv_data_ap.dy, 'r')\n",
    "plt.ylabel('SV (nT/yr)', fontsize=16)\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(sv_data.date, sv_data.dz, 'b', label='All data')\n",
    "plt.plot(sv_data.date, sv_data_ap.dz, 'r', label = 'ap < 7')\n",
    "plt.gca().xaxis_date()\n",
    "plt.ylabel('Year', fontsize=16)\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "io.ae_readfile('/Users/gracecox/Dropbox/DataDownloads/AE/AE_kyoto_raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "io.append_ae_data('/Users/gracecox/Dropbox/ae_index/HOURLY/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
